{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10.3 word2vec的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9.0\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import math\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data\n",
    "\n",
    "sys.path.append(\"..\") \n",
    "import d2lzh_pytorch as d2l\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.3.1 处理数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 'ptb.train.txt' in os.listdir(\"../../data/ptb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# sentences: 42068'"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../../data/ptb/ptb.train.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    # st是sentence的缩写\n",
    "    raw_dataset = [st.split() for st in lines]\n",
    "\n",
    "'# sentences: %d' % len(raw_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# tokens: 24 ['aer', 'banknote', 'berlitz', 'calloway', 'centrust', 'cluett', 'fromstein', 'gitano', 'guterman', 'hydro-quebec', 'ipo', 'kia', 'memotec', 'mlx', 'nahb', 'punts', 'rake', 'regatta', 'rubens', 'sim', 'snack-food', 'ssangyong', 'swapo', 'wachter']\n",
      "# tokens: 15 ['pierre', '<unk>', 'N', 'years', 'old', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'nov.', 'N']\n",
      "# tokens: 11 ['mr.', '<unk>', 'is', 'chairman', 'of', '<unk>', 'n.v.', 'the', 'dutch', 'publishing', 'group']\n"
     ]
    }
   ],
   "source": [
    "for st in raw_dataset[:3]:\n",
    "    print('# tokens:', len(st), st[:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3.1.1 建立词语索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tk是token的缩写\n",
    "counter = collections.Counter([tk for st in raw_dataset for tk in st])\n",
    "# counter.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为了计算简单，我们只保留在数据集中至少出现5次的词\n",
    "# 除去出现次数少于5的字符\n",
    "counter = dict(filter(lambda x: x[1] >= 5, counter.items()))\n",
    "# counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_token = [tk for tk, _ in counter.items()]\n",
    "# idx_to_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('unilab', 9857),\n",
       " 9858,\n",
       " {'pierre': 0,\n",
       "  '<unk>': 1,\n",
       "  'N': 2,\n",
       "  'years': 3,\n",
       "  'old': 4,\n",
       "  'will': 5,\n",
       "  'join': 6,\n",
       "  'the': 7,\n",
       "  'board': 8,\n",
       "  'as': 9,\n",
       "  'a': 10,\n",
       "  'nonexecutive': 11,\n",
       "  'director': 12,\n",
       "  'nov.': 13,\n",
       "  'mr.': 14,\n",
       "  'is': 15,\n",
       "  'chairman': 16,\n",
       "  'of': 17,\n",
       "  'n.v.': 18,\n",
       "  'dutch': 19,\n",
       "  'publishing': 20,\n",
       "  'group': 21,\n",
       "  'rudolph': 22,\n",
       "  'and': 23,\n",
       "  'former': 24,\n",
       "  'consolidated': 25,\n",
       "  'gold': 26,\n",
       "  'fields': 27,\n",
       "  'plc': 28,\n",
       "  'was': 29,\n",
       "  'named': 30,\n",
       "  'this': 31,\n",
       "  'british': 32,\n",
       "  'industrial': 33,\n",
       "  'conglomerate': 34,\n",
       "  'form': 35,\n",
       "  'asbestos': 36,\n",
       "  'once': 37,\n",
       "  'used': 38,\n",
       "  'to': 39,\n",
       "  'make': 40,\n",
       "  'kent': 41,\n",
       "  'cigarette': 42,\n",
       "  'filters': 43,\n",
       "  'has': 44,\n",
       "  'caused': 45,\n",
       "  'high': 46,\n",
       "  'percentage': 47,\n",
       "  'cancer': 48,\n",
       "  'deaths': 49,\n",
       "  'among': 50,\n",
       "  'workers': 51,\n",
       "  'exposed': 52,\n",
       "  'it': 53,\n",
       "  'more': 54,\n",
       "  'than': 55,\n",
       "  'ago': 56,\n",
       "  'researchers': 57,\n",
       "  'reported': 58,\n",
       "  'fiber': 59,\n",
       "  'unusually': 60,\n",
       "  'enters': 61,\n",
       "  'with': 62,\n",
       "  'even': 63,\n",
       "  'brief': 64,\n",
       "  'exposures': 65,\n",
       "  'causing': 66,\n",
       "  'symptoms': 67,\n",
       "  'that': 68,\n",
       "  'show': 69,\n",
       "  'up': 70,\n",
       "  'decades': 71,\n",
       "  'later': 72,\n",
       "  'said': 73,\n",
       "  'inc.': 74,\n",
       "  'unit': 75,\n",
       "  'new': 76,\n",
       "  'york-based': 77,\n",
       "  'corp.': 78,\n",
       "  'makes': 79,\n",
       "  'cigarettes': 80,\n",
       "  'stopped': 81,\n",
       "  'using': 82,\n",
       "  'in': 83,\n",
       "  'its': 84,\n",
       "  'although': 85,\n",
       "  'preliminary': 86,\n",
       "  'findings': 87,\n",
       "  'were': 88,\n",
       "  'year': 89,\n",
       "  'latest': 90,\n",
       "  'results': 91,\n",
       "  'appear': 92,\n",
       "  'today': 93,\n",
       "  \"'s\": 94,\n",
       "  'england': 95,\n",
       "  'journal': 96,\n",
       "  'medicine': 97,\n",
       "  'forum': 98,\n",
       "  'likely': 99,\n",
       "  'bring': 100,\n",
       "  'attention': 101,\n",
       "  'problem': 102,\n",
       "  'an': 103,\n",
       "  'story': 104,\n",
       "  'we': 105,\n",
       "  \"'re\": 106,\n",
       "  'talking': 107,\n",
       "  'about': 108,\n",
       "  'before': 109,\n",
       "  'anyone': 110,\n",
       "  'heard': 111,\n",
       "  'having': 112,\n",
       "  'any': 113,\n",
       "  'questionable': 114,\n",
       "  'properties': 115,\n",
       "  'there': 116,\n",
       "  'no': 117,\n",
       "  'our': 118,\n",
       "  'products': 119,\n",
       "  'now': 120,\n",
       "  'neither': 121,\n",
       "  'nor': 122,\n",
       "  'who': 123,\n",
       "  'studied': 124,\n",
       "  'aware': 125,\n",
       "  'research': 126,\n",
       "  'on': 127,\n",
       "  'smokers': 128,\n",
       "  'have': 129,\n",
       "  'useful': 130,\n",
       "  'information': 131,\n",
       "  'whether': 132,\n",
       "  'users': 133,\n",
       "  'are': 134,\n",
       "  'at': 135,\n",
       "  'risk': 136,\n",
       "  'james': 137,\n",
       "  'a.': 138,\n",
       "  'boston': 139,\n",
       "  'institute': 140,\n",
       "  'dr.': 141,\n",
       "  'led': 142,\n",
       "  'team': 143,\n",
       "  'from': 144,\n",
       "  'national': 145,\n",
       "  'medical': 146,\n",
       "  'schools': 147,\n",
       "  'harvard': 148,\n",
       "  'university': 149,\n",
       "  'spokeswoman': 150,\n",
       "  'very': 151,\n",
       "  'modest': 152,\n",
       "  'amounts': 153,\n",
       "  'making': 154,\n",
       "  'paper': 155,\n",
       "  'for': 156,\n",
       "  'early': 157,\n",
       "  '1950s': 158,\n",
       "  'replaced': 159,\n",
       "  'different': 160,\n",
       "  'type': 161,\n",
       "  'billion': 162,\n",
       "  'sold': 163,\n",
       "  'company': 164,\n",
       "  'men': 165,\n",
       "  'worked': 166,\n",
       "  'closely': 167,\n",
       "  'substance': 168,\n",
       "  'died': 169,\n",
       "  'three': 170,\n",
       "  'times': 171,\n",
       "  'expected': 172,\n",
       "  'number': 173,\n",
       "  'four': 174,\n",
       "  'five': 175,\n",
       "  'surviving': 176,\n",
       "  'diseases': 177,\n",
       "  'including': 178,\n",
       "  'recently': 179,\n",
       "  'total': 180,\n",
       "  'malignant': 181,\n",
       "  'lung': 182,\n",
       "  'far': 183,\n",
       "  'higher': 184,\n",
       "  'rate': 185,\n",
       "  'striking': 186,\n",
       "  'finding': 187,\n",
       "  'those': 188,\n",
       "  'us': 189,\n",
       "  'study': 190,\n",
       "  'west': 191,\n",
       "  'mass.': 192,\n",
       "  'factory': 193,\n",
       "  'appears': 194,\n",
       "  'be': 195,\n",
       "  'highest': 196,\n",
       "  'western': 197,\n",
       "  'industrialized': 198,\n",
       "  'countries': 199,\n",
       "  'he': 200,\n",
       "  'plant': 201,\n",
       "  'which': 202,\n",
       "  'owned': 203,\n",
       "  'by': 204,\n",
       "  '&': 205,\n",
       "  'co.': 206,\n",
       "  'under': 207,\n",
       "  'contract': 208,\n",
       "  'probably': 209,\n",
       "  'support': 210,\n",
       "  'argue': 211,\n",
       "  'u.s.': 212,\n",
       "  'should': 213,\n",
       "  'regulate': 214,\n",
       "  'class': 215,\n",
       "  'common': 216,\n",
       "  'kind': 217,\n",
       "  'found': 218,\n",
       "  'most': 219,\n",
       "  'other': 220,\n",
       "  'buildings': 221,\n",
       "  'one': 222,\n",
       "  'few': 223,\n",
       "  'nations': 224,\n",
       "  'does': 225,\n",
       "  \"n't\": 226,\n",
       "  'standard': 227,\n",
       "  'regulation': 228,\n",
       "  'smooth': 229,\n",
       "  'fibers': 230,\n",
       "  'such': 231,\n",
       "  'classified': 232,\n",
       "  'according': 233,\n",
       "  't.': 234,\n",
       "  'professor': 235,\n",
       "  'vermont': 236,\n",
       "  'college': 237,\n",
       "  'easily': 238,\n",
       "  'rejected': 239,\n",
       "  'body': 240,\n",
       "  'explained': 241,\n",
       "  'july': 242,\n",
       "  'environmental': 243,\n",
       "  'protection': 244,\n",
       "  'agency': 245,\n",
       "  'imposed': 246,\n",
       "  'gradual': 247,\n",
       "  'ban': 248,\n",
       "  'virtually': 249,\n",
       "  'all': 250,\n",
       "  'uses': 251,\n",
       "  'almost': 252,\n",
       "  'remaining': 253,\n",
       "  'outlawed': 254,\n",
       "  'made': 255,\n",
       "  'areas': 256,\n",
       "  'particularly': 257,\n",
       "  'dusty': 258,\n",
       "  'where': 259,\n",
       "  'dumped': 260,\n",
       "  'large': 261,\n",
       "  'imported': 262,\n",
       "  'material': 263,\n",
       "  'into': 264,\n",
       "  'huge': 265,\n",
       "  'poured': 266,\n",
       "  'cotton': 267,\n",
       "  'mixed': 268,\n",
       "  'dry': 269,\n",
       "  'process': 270,\n",
       "  'described': 271,\n",
       "  'clouds': 272,\n",
       "  'blue': 273,\n",
       "  'dust': 274,\n",
       "  'hung': 275,\n",
       "  'over': 276,\n",
       "  'parts': 277,\n",
       "  'though': 278,\n",
       "  'fans': 279,\n",
       "  'area': 280,\n",
       "  'question': 281,\n",
       "  'some': 282,\n",
       "  'managers': 283,\n",
       "  'contracted': 284,\n",
       "  'phillips': 285,\n",
       "  'vice': 286,\n",
       "  'president': 287,\n",
       "  'human': 288,\n",
       "  'resources': 289,\n",
       "  'but': 290,\n",
       "  'you': 291,\n",
       "  'recognize': 292,\n",
       "  'these': 293,\n",
       "  'events': 294,\n",
       "  'took': 295,\n",
       "  'place': 296,\n",
       "  'bearing': 297,\n",
       "  'work': 298,\n",
       "  'force': 299,\n",
       "  'yields': 300,\n",
       "  'money-market': 301,\n",
       "  'mutual': 302,\n",
       "  'funds': 303,\n",
       "  'continued': 304,\n",
       "  'slide': 305,\n",
       "  'amid': 306,\n",
       "  'signs': 307,\n",
       "  'portfolio': 308,\n",
       "  'expect': 309,\n",
       "  'further': 310,\n",
       "  'declines': 311,\n",
       "  'interest': 312,\n",
       "  'rates': 313,\n",
       "  'average': 314,\n",
       "  'seven-day': 315,\n",
       "  'compound': 316,\n",
       "  'yield': 317,\n",
       "  'taxable': 318,\n",
       "  'tracked': 319,\n",
       "  'money': 320,\n",
       "  'fund': 321,\n",
       "  'report': 322,\n",
       "  'eased': 323,\n",
       "  'fraction': 324,\n",
       "  'point': 325,\n",
       "  'week': 326,\n",
       "  'ended': 327,\n",
       "  'tuesday': 328,\n",
       "  'assume': 329,\n",
       "  'reinvestment': 330,\n",
       "  'dividends': 331,\n",
       "  'current': 332,\n",
       "  'continues': 333,\n",
       "  'maturity': 334,\n",
       "  \"'\": 335,\n",
       "  'investments': 336,\n",
       "  'day': 337,\n",
       "  'days': 338,\n",
       "  'longest': 339,\n",
       "  'since': 340,\n",
       "  'august': 341,\n",
       "  'donoghue': 342,\n",
       "  'longer': 343,\n",
       "  'maturities': 344,\n",
       "  'thought': 345,\n",
       "  'indicate': 346,\n",
       "  'declining': 347,\n",
       "  'because': 348,\n",
       "  'they': 349,\n",
       "  'permit': 350,\n",
       "  'retain': 351,\n",
       "  'relatively': 352,\n",
       "  'period': 353,\n",
       "  'shorter': 354,\n",
       "  'considered': 355,\n",
       "  'sign': 356,\n",
       "  'rising': 357,\n",
       "  'can': 358,\n",
       "  'capture': 359,\n",
       "  'sooner': 360,\n",
       "  'open': 361,\n",
       "  'only': 362,\n",
       "  'institutions': 363,\n",
       "  'stronger': 364,\n",
       "  'indicator': 365,\n",
       "  'watch': 366,\n",
       "  'market': 367,\n",
       "  'reached': 368,\n",
       "  'nevertheless': 369,\n",
       "  'editor': 370,\n",
       "  'may': 371,\n",
       "  'again': 372,\n",
       "  'down': 373,\n",
       "  'recent': 374,\n",
       "  'rises': 375,\n",
       "  'short-term': 376,\n",
       "  'six-month': 377,\n",
       "  'treasury': 378,\n",
       "  'bills': 379,\n",
       "  'monday': 380,\n",
       "  'auction': 381,\n",
       "  'example': 382,\n",
       "  'rose': 383,\n",
       "  'despite': 384,\n",
       "  'investors': 385,\n",
       "  'continue': 386,\n",
       "  'pour': 387,\n",
       "  'cash': 388,\n",
       "  'assets': 389,\n",
       "  'grew': 390,\n",
       "  '$': 391,\n",
       "  'during': 392,\n",
       "  'typically': 393,\n",
       "  'money-fund': 394,\n",
       "  'beat': 395,\n",
       "  'comparable': 396,\n",
       "  'vary': 397,\n",
       "  'go': 398,\n",
       "  'after': 399,\n",
       "  'top': 400,\n",
       "  'currently': 401,\n",
       "  'yielding': 402,\n",
       "  'well': 403,\n",
       "  'dreyfus': 404,\n",
       "  'world-wide': 405,\n",
       "  'dollar': 406,\n",
       "  'had': 407,\n",
       "  'earlier': 408,\n",
       "  'invests': 409,\n",
       "  'heavily': 410,\n",
       "  'dollar-denominated': 411,\n",
       "  'securities': 412,\n",
       "  'overseas': 413,\n",
       "  'management': 414,\n",
       "  'fees': 415,\n",
       "  'boosts': 416,\n",
       "  'simple': 417,\n",
       "  '30-day': 418,\n",
       "  'fell': 419,\n",
       "  'slid': 420,\n",
       "  'j.p.': 421,\n",
       "  'grace': 422,\n",
       "  'holds': 423,\n",
       "  'elected': 424,\n",
       "  'succeeds': 425,\n",
       "  'd.': 426,\n",
       "  'formerly': 427,\n",
       "  'resigned': 428,\n",
       "  'energy': 429,\n",
       "  'seven': 430,\n",
       "  'seats': 431,\n",
       "  'pacific': 432,\n",
       "  'first': 433,\n",
       "  'financial': 434,\n",
       "  'shareholders': 435,\n",
       "  'approved': 436,\n",
       "  'acquisition': 437,\n",
       "  'royal': 438,\n",
       "  'ltd.': 439,\n",
       "  'toronto': 440,\n",
       "  'share': 441,\n",
       "  'or': 442,\n",
       "  'million': 443,\n",
       "  'thrift': 444,\n",
       "  'holding': 445,\n",
       "  'expects': 446,\n",
       "  'obtain': 447,\n",
       "  'regulatory': 448,\n",
       "  'approval': 449,\n",
       "  'complete': 450,\n",
       "  'transaction': 451,\n",
       "  'year-end': 452,\n",
       "  'international': 453,\n",
       "  'completed': 454,\n",
       "  'sale': 455,\n",
       "  'controls': 456,\n",
       "  'operations': 457,\n",
       "  's.p': 458,\n",
       "  'italian': 459,\n",
       "  'state-owned': 460,\n",
       "  'interests': 461,\n",
       "  'mechanical': 462,\n",
       "  'engineering': 463,\n",
       "  'industry': 464,\n",
       "  'based': 465,\n",
       "  'ohio': 466,\n",
       "  'computerized': 467,\n",
       "  'systems': 468,\n",
       "  'employs': 469,\n",
       "  'people': 470,\n",
       "  'annual': 471,\n",
       "  'revenue': 472,\n",
       "  'federal': 473,\n",
       "  'government': 474,\n",
       "  'suspended': 475,\n",
       "  'sales': 476,\n",
       "  'savings': 477,\n",
       "  'bonds': 478,\n",
       "  'congress': 479,\n",
       "  'lifted': 480,\n",
       "  'ceiling': 481,\n",
       "  'debt': 482,\n",
       "  'until': 483,\n",
       "  'acts': 484,\n",
       "  'authority': 485,\n",
       "  'issue': 486,\n",
       "  'obligations': 487,\n",
       "  'borrowing': 488,\n",
       "  'dropped': 489,\n",
       "  'midnight': 490,\n",
       "  'trillion': 491,\n",
       "  'legislation': 492,\n",
       "  'lift': 493,\n",
       "  'fight': 494,\n",
       "  'cutting': 495,\n",
       "  'capital-gains': 496,\n",
       "  'taxes': 497,\n",
       "  'house': 498,\n",
       "  'voted': 499,\n",
       "  'raise': 500,\n",
       "  'senate': 501,\n",
       "  'act': 502,\n",
       "  'next': 503,\n",
       "  'earliest': 504,\n",
       "  'default': 505,\n",
       "  'if': 506,\n",
       "  'then': 507,\n",
       "  'clark': 508,\n",
       "  'j.': 509,\n",
       "  'senior': 510,\n",
       "  'general': 511,\n",
       "  'manager': 512,\n",
       "  'marketing': 513,\n",
       "  'arm': 514,\n",
       "  'japanese': 515,\n",
       "  'auto': 516,\n",
       "  'maker': 517,\n",
       "  'mazda': 518,\n",
       "  'motor': 519,\n",
       "  'corp': 520,\n",
       "  'position': 521,\n",
       "  'oversee': 522,\n",
       "  'service': 523,\n",
       "  'previously': 524,\n",
       "  'chrysler': 525,\n",
       "  'division': 526,\n",
       "  'been': 527,\n",
       "  'executive': 528,\n",
       "  'when': 529,\n",
       "  'time': 530,\n",
       "  'their': 531,\n",
       "  'nation': 532,\n",
       "  'manufacturing': 533,\n",
       "  'jet': 534,\n",
       "  'off': 535,\n",
       "  'resort': 536,\n",
       "  'towns': 537,\n",
       "  'like': 538,\n",
       "  'hot': 539,\n",
       "  'springs': 540,\n",
       "  'not': 541,\n",
       "  'association': 542,\n",
       "  'manufacturers': 543,\n",
       "  'settled': 544,\n",
       "  'capital': 545,\n",
       "  'indianapolis': 546,\n",
       "  'fall': 547,\n",
       "  'meeting': 548,\n",
       "  'city': 549,\n",
       "  'decided': 550,\n",
       "  'treat': 551,\n",
       "  'guests': 552,\n",
       "  'royalty': 553,\n",
       "  'rock': 554,\n",
       "  'stars': 555,\n",
       "  'owners': 556,\n",
       "  'idea': 557,\n",
       "  'course': 558,\n",
       "  'prove': 559,\n",
       "  'corporate': 560,\n",
       "  'decision': 561,\n",
       "  'makers': 562,\n",
       "  'buckle': 563,\n",
       "  'belt': 564,\n",
       "  'so': 565,\n",
       "  'good': 566,\n",
       "  'expand': 567,\n",
       "  'receiving': 568,\n",
       "  'end': 569,\n",
       "  'message': 570,\n",
       "  'officials': 571,\n",
       "  'giants': 572,\n",
       "  'du': 573,\n",
       "  'pont': 574,\n",
       "  'along': 575,\n",
       "  'lesser': 576,\n",
       "  'steel': 577,\n",
       "  'valley': 578,\n",
       "  'queen': 579,\n",
       "  'executives': 580,\n",
       "  'joined': 581,\n",
       "  'mayor': 582,\n",
       "  'william': 583,\n",
       "  'h.': 584,\n",
       "  'iii': 585,\n",
       "  'evening': 586,\n",
       "  'guest': 587,\n",
       "  'victor': 588,\n",
       "  'champagne': 589,\n",
       "  'followed': 590,\n",
       "  'morning': 591,\n",
       "  'police': 592,\n",
       "  'wives': 593,\n",
       "  'traffic': 594,\n",
       "  'red': 595,\n",
       "  'lights': 596,\n",
       "  'governor': 597,\n",
       "  'could': 598,\n",
       "  'welcomed': 599,\n",
       "  'special': 600,\n",
       "  'buffet': 601,\n",
       "  'breakfast': 602,\n",
       "  'held': 603,\n",
       "  'museum': 604,\n",
       "  'food': 605,\n",
       "  'drinks': 606,\n",
       "  'banned': 607,\n",
       "  'everyday': 608,\n",
       "  'visitors': 609,\n",
       "  'honor': 610,\n",
       "  'out': 611,\n",
       "  'drivers': 612,\n",
       "  'crews': 613,\n",
       "  'official': 614,\n",
       "  'announcer': 615,\n",
       "  'exhibition': 616,\n",
       "  'race': 617,\n",
       "  'fortune': 618,\n",
       "  'cars': 619,\n",
       "  'pointed': 620,\n",
       "  'still': 621,\n",
       "  'space': 622,\n",
       "  'machines': 623,\n",
       "  'another': 624,\n",
       "  'sponsor': 625,\n",
       "  'name': 626,\n",
       "  'two': 627,\n",
       "  'back': 628,\n",
       "  'downtown': 629,\n",
       "  'squeezed': 630,\n",
       "  'meetings': 631,\n",
       "  'hotel': 632,\n",
       "  'buses': 633,\n",
       "  'dinner': 634,\n",
       "  'block': 635,\n",
       "  'away': 636,\n",
       "  'indiana': 637,\n",
       "  'nine': 638,\n",
       "  'hottest': 639,\n",
       "  'chefs': 640,\n",
       "  'town': 641,\n",
       "  'fed': 642,\n",
       "  'them': 643,\n",
       "  'knowing': 644,\n",
       "  'free': 645,\n",
       "  'eat': 646,\n",
       "  'gave': 647,\n",
       "  'standing': 648,\n",
       "  'say': 649,\n",
       "  'treatment': 650,\n",
       "  'return': 651,\n",
       "  'future': 652,\n",
       "  'looking': 653,\n",
       "  'forward': 654,\n",
       "  'winter': 655,\n",
       "  'february': 656,\n",
       "  'south': 657,\n",
       "  'korea': 658,\n",
       "  'registered': 659,\n",
       "  'trade': 660,\n",
       "  'deficit': 661,\n",
       "  'october': 662,\n",
       "  'reflecting': 663,\n",
       "  'country': 664,\n",
       "  'economic': 665,\n",
       "  'figures': 666,\n",
       "  'released': 667,\n",
       "  'wednesday': 668,\n",
       "  'ministry': 669,\n",
       "  'showed': 670,\n",
       "  'fifth': 671,\n",
       "  'monthly': 672,\n",
       "  'setback': 673,\n",
       "  'casting': 674,\n",
       "  'cloud': 675,\n",
       "  'economy': 676,\n",
       "  'exports': 677,\n",
       "  'stood': 678,\n",
       "  'mere': 679,\n",
       "  'increase': 680,\n",
       "  'while': 681,\n",
       "  'imports': 682,\n",
       "  'increased': 683,\n",
       "  'sharply': 684,\n",
       "  'last': 685,\n",
       "  'boom': 686,\n",
       "  'began': 687,\n",
       "  'prolonged': 688,\n",
       "  'labor': 689,\n",
       "  'disputes': 690,\n",
       "  'conflicts': 691,\n",
       "  'sluggish': 692,\n",
       "  'would': 693,\n",
       "  'remain': 694,\n",
       "  'target': 695,\n",
       "  'gloomy': 696,\n",
       "  'forecast': 697,\n",
       "  'recorded': 698,\n",
       "  'surplus': 699,\n",
       "  'january': 700,\n",
       "  'accumulated': 701,\n",
       "  'same': 702,\n",
       "  'newsweek': 703,\n",
       "  'trying': 704,\n",
       "  'keep': 705,\n",
       "  'pace': 706,\n",
       "  'rival': 707,\n",
       "  'magazine': 708,\n",
       "  'announced': 709,\n",
       "  'advertising': 710,\n",
       "  'introduce': 711,\n",
       "  'incentive': 712,\n",
       "  'plan': 713,\n",
       "  'advertisers': 714,\n",
       "  'ad': 715,\n",
       "  'washington': 716,\n",
       "  'post': 717,\n",
       "  'second': 718,\n",
       "  'offered': 719,\n",
       "  'plans': 720,\n",
       "  'give': 721,\n",
       "  'discounts': 722,\n",
       "  'maintaining': 723,\n",
       "  'increasing': 724,\n",
       "  'spending': 725,\n",
       "  'become': 726,\n",
       "  'permanent': 727,\n",
       "  'news': 728,\n",
       "  'underscore': 729,\n",
       "  'fierce': 730,\n",
       "  'competition': 731,\n",
       "  'between': 732,\n",
       "  'warner': 733,\n",
       "  'b.': 734,\n",
       "  'world': 735,\n",
       "  'alan': 736,\n",
       "  'full': 737,\n",
       "  'page': 738,\n",
       "  'cost': 739,\n",
       "  'mid-october': 740,\n",
       "  'lowered': 741,\n",
       "  'guaranteed': 742,\n",
       "  'circulation': 743,\n",
       "  'base': 744,\n",
       "  'lower': 745,\n",
       "  'effectively': 746,\n",
       "  'per': 747,\n",
       "  'subscriber': 748,\n",
       "  'costs': 749,\n",
       "  'yet': 750,\n",
       "  'announce': 751,\n",
       "  'credit': 752,\n",
       "  'credits': 753,\n",
       "  'renewal': 754,\n",
       "  'reward': 755,\n",
       "  'bonuses': 756,\n",
       "  'meet': 757,\n",
       "  'exceed': 758,\n",
       "  'long': 759,\n",
       "  'spent': 760,\n",
       "  'attempt': 761,\n",
       "  'shore': 762,\n",
       "  'decline': 763,\n",
       "  'pages': 764,\n",
       "  'months': 765,\n",
       "  'totaled': 766,\n",
       "  'drop': 767,\n",
       "  'publishers': 768,\n",
       "  'bureau': 769,\n",
       "  'what': 770,\n",
       "  'matters': 771,\n",
       "  'paying': 772,\n",
       "  'department': 773,\n",
       "  'doing': 774,\n",
       "  'fine': 775,\n",
       "  'both': 776,\n",
       "  'gaining': 777,\n",
       "  'without': 778,\n",
       "  'heavy': 779,\n",
       "  'use': 780,\n",
       "  'electronic': 781,\n",
       "  'subscribers': 782,\n",
       "  'telephones': 783,\n",
       "  'watches': 784,\n",
       "  'however': 785,\n",
       "  'none': 786,\n",
       "  'big': 787,\n",
       "  'gains': 788,\n",
       "  'audit': 789,\n",
       "  'largest': 790,\n",
       "  'decrease': 791,\n",
       "  'six': 792,\n",
       "  'flat': 793,\n",
       "  'electric': 794,\n",
       "  'system': 795,\n",
       "  'bowed': 796,\n",
       "  'bidding': 797,\n",
       "  'public': 798,\n",
       "  'hampshire': 799,\n",
       "  'saying': 800,\n",
       "  'risks': 801,\n",
       "  'too': 802,\n",
       "  'potential': 803,\n",
       "  'justify': 804,\n",
       "  'offer': 805,\n",
       "  'move': 806,\n",
       "  'leaves': 807,\n",
       "  'united': 808,\n",
       "  'illuminating': 809,\n",
       "  'northeast': 810,\n",
       "  'utilities': 811,\n",
       "  'outside': 812,\n",
       "  'bidders': 813,\n",
       "  'ps': 814,\n",
       "  'also': 815,\n",
       "  'proposed': 816,\n",
       "  'internal': 817,\n",
       "  'reorganization': 818,\n",
       "  'chapter': 819,\n",
       "  'bankruptcy': 820,\n",
       "  'proceedings': 821,\n",
       "  'independent': 822,\n",
       "  'acquire': 823,\n",
       "  'below': 824,\n",
       "  'value': 825,\n",
       "  'places': 826,\n",
       "  'bid': 827,\n",
       "  'says': 828,\n",
       "  'worth': 829,\n",
       "  'haven': 830,\n",
       "  'conn.': 831,\n",
       "  'hartford': 832,\n",
       "  'conn': 833,\n",
       "  'n.h.': 834,\n",
       "  'values': 835,\n",
       "  'john': 836,\n",
       "  'rowe': 837,\n",
       "  'chief': 838,\n",
       "  'officer': 839,\n",
       "  'equity': 840,\n",
       "  'suffer': 841,\n",
       "  'forecasts': 842,\n",
       "  'related': 843,\n",
       "  'growth': 844,\n",
       "  'electricity': 845,\n",
       "  'demand': 846,\n",
       "  'improved': 847,\n",
       "  'operating': 848,\n",
       "  'did': 849,\n",
       "  'come': 850,\n",
       "  'true': 851,\n",
       "  'raising': 852,\n",
       "  'seemed': 853,\n",
       "  'substantial': 854,\n",
       "  'persistent': 855,\n",
       "  'rewards': 856,\n",
       "  'way': 857,\n",
       "  'got': 858,\n",
       "  'hard': 859,\n",
       "  'take': 860,\n",
       "  'added': 861,\n",
       "  'noted': 862,\n",
       "  'political': 863,\n",
       "  'concerns': 864,\n",
       "  'worried': 865,\n",
       "  'matter': 866,\n",
       "  'owns': 867,\n",
       "  'emerges': 868,\n",
       "  'attracts': 869,\n",
       "  'just': 870,\n",
       "  'factors': 871,\n",
       "  'withdraw': 872,\n",
       "  'wilbur': 873,\n",
       "  'ross': 874,\n",
       "  'jr.': 875,\n",
       "  'rothschild': 876,\n",
       "  'adviser': 877,\n",
       "  'troubled': 878,\n",
       "  'holders': 879,\n",
       "  'withdrawal': 880,\n",
       "  'might': 881,\n",
       "  'speed': 882,\n",
       "  'fact': 883,\n",
       "  'increases': 884,\n",
       "  'against': 885,\n",
       "  'around': 886,\n",
       "  'complicated': 887,\n",
       "  'negotiations': 888,\n",
       "  'state': 889,\n",
       "  'asserted': 890,\n",
       "  'field': 891,\n",
       "  'less': 892,\n",
       "  'separately': 893,\n",
       "  'commission': 894,\n",
       "  'turned': 895,\n",
       "  'request': 896,\n",
       "  'seeking': 897,\n",
       "  'possible': 898,\n",
       "  'purchase': 899,\n",
       "  'hopes': 900,\n",
       "  'review': 901,\n",
       "  'summer': 902,\n",
       "  'court': 903,\n",
       "  'shares': 904,\n",
       "  'closed': 905,\n",
       "  'yesterday': 906,\n",
       "  'cents': 907,\n",
       "  'york': 908,\n",
       "  'stock': 909,\n",
       "  'exchange': 910,\n",
       "  'composite': 911,\n",
       "  'trading': 912,\n",
       "  'norman': 913,\n",
       "  'toys': 914,\n",
       "  'r': 915,\n",
       "  'frederick': 916,\n",
       "  'banking': 917,\n",
       "  'directors': 918,\n",
       "  'consumer': 919,\n",
       "  'electronics': 920,\n",
       "  'appliances': 921,\n",
       "  'retailing': 922,\n",
       "  'chain': 923,\n",
       "  'succeed': 924,\n",
       "  'daniel': 925,\n",
       "  'm.': 926,\n",
       "  'retired': 927,\n",
       "  'circuit': 928,\n",
       "  'robert': 929,\n",
       "  'r.': 930,\n",
       "  'undersecretary': 931,\n",
       "  'commonwealth': 932,\n",
       "  'edison': 933,\n",
       "  'ordered': 934,\n",
       "  'refund': 935,\n",
       "  'illegal': 936,\n",
       "  'collected': 937,\n",
       "  'overruns': 938,\n",
       "  'nuclear': 939,\n",
       "  'power': 940,\n",
       "  'illinois': 941,\n",
       "  'commerce': 942,\n",
       "  'groups': 943,\n",
       "  'ever': 944,\n",
       "  'required': 945,\n",
       "  'local': 946,\n",
       "  'utility': 947,\n",
       "  'judge': 948,\n",
       "  'richard': 949,\n",
       "  'curry': 950,\n",
       "  'refunds': 951,\n",
       "  'each': 952,\n",
       "  'customers': 953,\n",
       "  'received': 954,\n",
       "  'april': 955,\n",
       "  'moved': 956,\n",
       "  'begin': 957,\n",
       "  'feb.': 958,\n",
       "  'appeals': 959,\n",
       "  'attempts': 960,\n",
       "  'his': 961,\n",
       "  'order': 962,\n",
       "  'pool': 963,\n",
       "  'through': 964,\n",
       "  'round': 965,\n",
       "  'already': 966,\n",
       "  'appealing': 967,\n",
       "  'underlying': 968,\n",
       "  'considering': 969,\n",
       "  'exact': 970,\n",
       "  'amount': 971,\n",
       "  'determined': 972,\n",
       "  'actual': 973,\n",
       "  'dec.': 974,\n",
       "  'ruling': 975,\n",
       "  'slash': 976,\n",
       "  'earnings': 977,\n",
       "  'spokesman': 978,\n",
       "  'tracking': 979,\n",
       "  'whose': 980,\n",
       "  'addresses': 981,\n",
       "  'changed': 982,\n",
       "  'past': 983,\n",
       "  'administrative': 984,\n",
       "  'nightmare': 985,\n",
       "  'near': 986,\n",
       "  'ill.': 987,\n",
       "  'disputed': 988,\n",
       "  'pay': 989,\n",
       "  'courts': 990,\n",
       "  'upheld': 991,\n",
       "  'challenge': 992,\n",
       "  'supreme': 993,\n",
       "  'construction': 994,\n",
       "  'expenses': 995,\n",
       "  'collecting': 996,\n",
       "  'subject': 997,\n",
       "  'ruled': 998,\n",
       "  'plus': 999,\n",
       "  ...})"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_to_idx = {tk: idx for idx, tk in enumerate(idx_to_token)}\n",
    "list(token_to_idx.items())[-1], len(token_to_idx), token_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 2]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = [[token_to_idx[tk] for tk in st if tk in token_to_idx]\n",
    "           for st in raw_dataset]\n",
    "dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# tokens: 887100'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_tokens = sum([len(st) for st in dataset])\n",
    "'# tokens: %d' % num_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3.1.2 二次采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# tokens: 375370'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def discard(idx):\n",
    "    return random.uniform(0, 1) < 1 - math.sqrt(\n",
    "        1e-4 / counter[idx_to_token[idx]] * num_tokens)\n",
    "\n",
    "\n",
    "subsampled_dataset = [[tk for tk in st if not discard(tk)] for st in dataset]\n",
    "'# tokens: %d' % sum([len(st) for st in subsampled_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['n.v.', 'dutch', 'publishing']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[idx_to_token[idx] for idx in subsampled_dataset[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# the: before=50770, after=2107'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compare_counts(token):\n",
    "    return '# %s: before=%d, after=%d' % (token, sum(\n",
    "        [st.count(token_to_idx[token]) for st in dataset]), sum(\n",
    "        [st.count(token_to_idx[token]) for st in subsampled_dataset]))\n",
    "\n",
    "compare_counts('the')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# join: before=45, after=45'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_counts('join')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# mom: before=6, after=6'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_counts('mom')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3.1.3 提取中心词和背景词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_centers_and_contexts(dataset, max_window_size):\n",
    "    centers, contexts = [], []\n",
    "    for st in dataset:\n",
    "        if len(st) < 2:  # 每个句子至少要有2个词才可能组成一对“中心词-背景词”\n",
    "            continue\n",
    "        centers += st\n",
    "        for center_i in range(len(st)):\n",
    "            window_size = random.randint(1, max_window_size)\n",
    "            indices = list(range(max(0, center_i - window_size),\n",
    "                                 min(len(st), center_i + 1 + window_size)))\n",
    "            indices.remove(center_i)  # 将中心词排除在背景词之外\n",
    "            contexts.append([st[idx] for idx in indices])\n",
    "    return centers, contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset [[0, 1, 2, 3, 4, 5, 6], [7, 8, 9]]\n",
      "center 0 has contexts [1, 2]\n",
      "center 1 has contexts [0, 2, 3]\n",
      "center 2 has contexts [0, 1, 3, 4]\n",
      "center 3 has contexts [2, 4]\n",
      "center 4 has contexts [3, 5]\n",
      "center 5 has contexts [3, 4, 6]\n",
      "center 6 has contexts [4, 5]\n",
      "center 7 has contexts [8, 9]\n",
      "center 8 has contexts [7, 9]\n",
      "center 9 has contexts [7, 8]\n"
     ]
    }
   ],
   "source": [
    "tiny_dataset = [list(range(7)), list(range(7, 10))]\n",
    "print('dataset', tiny_dataset)\n",
    "for center, context in zip(*get_centers_and_contexts(tiny_dataset, 2)):\n",
    "    print('center', center, 'has contexts', context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_centers, all_contexts = get_centers_and_contexts(subsampled_dataset, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17, [133, 134, 138, 139, 48, 140])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_centers[100], all_contexts[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('boston', ['a.', 'of', 'cancer', 'institute'])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_to_token[all_centers[101]], [idx_to_token[idx] for idx in all_contexts[101]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.3.2 负采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx_to_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_weights = [counter[w]**0.75 for w in idx_to_token]\n",
    "# sampling_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_negatives(all_contexts, sampling_weights, K):\n",
    "    all_negatives, neg_candidates, i = [], [], 0\n",
    "    population = list(range(len(sampling_weights)))\n",
    "    for contexts in all_contexts:\n",
    "        negatives = []\n",
    "        while len(negatives) < len(contexts) * K:\n",
    "            if i == len(neg_candidates):\n",
    "                # 根据每个词的权重（sampling_weights）随机生成k个词的索引作为噪声词。\n",
    "                # 为了高效计算，可以将k设得稍大一点\n",
    "                i, neg_candidates = 0, random.choices(\n",
    "                    population, sampling_weights, k=int(1e5))\n",
    "            neg, i = neg_candidates[i], i + 1\n",
    "            # 噪声词不能是背景词\n",
    "            if neg not in set(contexts):\n",
    "                negatives.append(neg)\n",
    "        all_negatives.append(negatives)\n",
    "    return all_negatives\n",
    "\n",
    "all_negatives = get_negatives(all_contexts, sampling_weights, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,\n",
       " [4, 6, 11, 12],\n",
       " [222,\n",
       "  6515,\n",
       "  9735,\n",
       "  3592,\n",
       "  7507,\n",
       "  7705,\n",
       "  7,\n",
       "  191,\n",
       "  1739,\n",
       "  2147,\n",
       "  693,\n",
       "  603,\n",
       "  94,\n",
       "  8308,\n",
       "  442,\n",
       "  385,\n",
       "  4303,\n",
       "  6247,\n",
       "  182,\n",
       "  163])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_centers[0], all_contexts[0], all_negatives[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.3.3 读取数据\n",
    "* Basically, the collate_fn receives a list of tuples if your `__getitem__` function from a Dataset subclass returns a tuple, or just a normal list if your Dataset subclass returns only one element. Its main objective is to create your batch without spending much time implementing it manually. Try to see it as a glue that you specify the way examples stick together in a batch. If you don’t use it, PyTorch only put batch_size examples together as you would using torch.stack (not exactly it, but it is simple like that).\n",
    "\n",
    "* Suppose for example, you want to create batches of a list of varying dimension tensors. The below code pads sequences with 0 until the maximum sequence size of the batch, that is why we need the `collate_fn`, because a standard batching algorithm (simply using `torch.stack`) won’t work in this case, and we need to manually pad different sequences with variable length to the same size before creating the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pierre'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_to_token[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(data):\n",
    "    \"\"\"用作DataLoader的参数collate_fn: 输入是个长为batchsize的list, list中的每个元素都是__getitem__得到的结果\"\"\"\n",
    "    max_len = max(len(c) + len(n) for _, c, n in data)\n",
    "#     print(f\"max_len - {max_len}\")\n",
    "    centers, contexts_negatives, masks, labels = [], [], [], []\n",
    "    for center, context, negative in data:\n",
    "        cur_len = len(context) + len(negative)\n",
    "        centers += [center]\n",
    "#         print(f\"centers - {centers}\")\n",
    "        contexts_negatives += [context + negative + [0] * (max_len - cur_len)]\n",
    "        masks += [[1] * cur_len + [0] * (max_len - cur_len)]\n",
    "        labels += [[1] * len(context) + [0] * (max_len - len(context))]\n",
    "    return (torch.tensor(centers).view(-1, 1), torch.tensor(contexts_negatives),\n",
    "            torch.tensor(masks), torch.tensor(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "\n",
    "example_data = zip(\n",
    "    all_centers[:batch_size], \n",
    "    all_contexts[:batch_size], \n",
    "    all_negatives[:batch_size]\n",
    ")\n",
    "\n",
    "# for center, context, negative in example_data:\n",
    "#     print(f\"{center}\\n{context}\\n{negative}\")\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# batchify(list(example_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, centers, contexts, negatives):\n",
    "        assert len(centers) == len(contexts) == len(negatives)\n",
    "        self.centers = centers\n",
    "        self.contexts = contexts\n",
    "        self.negatives = negatives\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (self.centers[index], self.contexts[index], self.negatives[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "num_workers = 0 if sys.platform.startswith('win32') else 16\n",
    "\n",
    "dataset = MyDataset(\n",
    "    all_centers,\n",
    "    all_contexts,\n",
    "    all_negatives\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(dataset), dataset[0], num_workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter = Data.DataLoader(\n",
    "    dataset, \n",
    "    batch_size, \n",
    "    shuffle=True,\n",
    "    collate_fn=batchify,\n",
    "#     num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "centers shape: torch.Size([512, 1])\n",
      "contexts_negatives shape: torch.Size([512, 60])\n",
      "masks shape: torch.Size([512, 60])\n",
      "labels shape: torch.Size([512, 60])\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(data_iter):\n",
    "    for name, data in zip(['centers', 'contexts_negatives', 'masks',\n",
    "                           'labels'], batch):\n",
    "        print(name, 'shape:', data.shape)\n",
    "#     print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.3.4 跳字模型\n",
    "### 10.3.4.1 嵌入层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9858, 100])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_embeddings = len(idx_to_token)\n",
    "embed_size = 100\n",
    "\n",
    "embed = nn.Embedding(\n",
    "    num_embeddings=num_embeddings, \n",
    "    embedding_dim=embed_size\n",
    ")\n",
    "embed.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 100])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.long)\n",
    "embed(x).shape\n",
    "# embed(x)[0][0] == embed.weight[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 1, 100]), torch.Size([3, 10, 100]))"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = torch.zeros((3, 1), dtype=torch.long)\n",
    "o = torch.ones((3, 10), dtype=torch.long)\n",
    "\n",
    "v = embed(c) \n",
    "u = embed(o)\n",
    "\n",
    "v.shape, u.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 10])"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.bmm(v, u.permute(0, 2, 1)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3.4.2 小批量乘法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 1, 6]),\n",
       " tensor([[[1., 1., 1., 1.]],\n",
       " \n",
       "         [[1., 1., 1., 1.]]]))"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.ones((2, 1, 4))\n",
    "Y = torch.ones((2, 4, 6))\n",
    "torch.bmm(X, Y).shape, X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3.4.3 跳字模型前向计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skip_gram(center, contexts_and_negatives, embed_v, embed_u):\n",
    "    v = embed_v(center)\n",
    "    u = embed_u(contexts_and_negatives)\n",
    "    pred = torch.bmm(v, u.permute(0, 2, 1))\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.3.5 训练模型\n",
    "### 10.3.5.1 二元交叉熵损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SigmoidBinaryCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self): # none mean sum\n",
    "        super(SigmoidBinaryCrossEntropyLoss, self).__init__()\n",
    "    def forward(self, inputs, targets, mask=None):\n",
    "        \"\"\"\n",
    "        input – Tensor shape: (batch_size, len)\n",
    "        target – Tensor of the same shape as input\n",
    "        \"\"\"\n",
    "        inputs, targets, mask = inputs.float(), targets.float(), mask.float()\n",
    "        res = nn.functional.binary_cross_entropy_with_logits(inputs, targets, reduction=\"none\", weight=mask)\n",
    "        return res.mean(dim=1)\n",
    "\n",
    "loss = SigmoidBinaryCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "值得一提的是，我们可以通过掩码变量指定小批量中参与损失函数计算的部分预测值和标签：当掩码为1时，相应位置的预测值和标签将参与损失函数的计算；当掩码为0时，相应位置的预测值和标签则不参与损失函数的计算。我们之前提到，掩码变量可用于避免填充项对损失函数计算的影响。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8740, 1.2100])"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = torch.tensor([[1.5, 0.3, -1, 2], [1.1, -0.6, 2.2, 0.4]], dtype = torch.float)\n",
    "# 标签变量label中的1和0分别代表背景词和噪声词\n",
    "label = torch.tensor([[1, 0, 0, 0], [1, 1, 0, 0]], dtype = torch.float)\n",
    "mask = torch.tensor([[1, 1, 1, 1], [1, 1, 1, 0]])  # 掩码变量\n",
    "loss(pred, label, mask) * mask.shape[1] / mask.float().sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.4960, 3.6299])"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(pred, label, mask)* mask.shape[1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, tensor([4., 3.]))"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.shape[1], mask.float().sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.8740), tensor(1.2100))"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.binary_cross_entropy_with_logits(pred[0], label[0]), F.binary_cross_entropy_with_logits(pred[1], label[1], weight=mask[1])*4/3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3.5.2 初始化模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9858"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(idx_to_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_embeddings = len(idx_to_token)\n",
    "embed_size = 100\n",
    "net = nn.Sequential(\n",
    "    nn.Embedding(num_embeddings=num_embeddings, embedding_dim=embed_size),\n",
    "    nn.Embedding(num_embeddings=num_embeddings, embedding_dim=embed_size)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3.5.3 定义训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, lr, num_epochs):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(\"train on\", device)\n",
    "    net = net.to(device)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    for epoch in range(num_epochs):\n",
    "        start, l_sum, n = time.time(), 0.0, 0\n",
    "        for batch in data_iter:\n",
    "            center, context_negative, mask, label = [d.to(device) for d in batch]\n",
    "            \n",
    "            pred = skip_gram(center, context_negative, net[0], net[1])\n",
    "            \n",
    "            # 使用掩码变量mask来避免填充项对损失函数计算的影响\n",
    "            l = (loss(pred.view(label.shape), label, mask) *\n",
    "                 mask.shape[1] / mask.float().sum(dim=1)).mean() # 一个batch的平均loss\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            l_sum += l.cpu().item()\n",
    "            n += 1\n",
    "        print('epoch %d, loss %.2f, time %.2fs'\n",
    "              % (epoch + 1, l_sum / n, time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on cpu\n",
      "epoch 1, loss 1.98, time 39.84s\n",
      "epoch 2, loss 0.62, time 40.26s\n",
      "epoch 3, loss 0.45, time 39.43s\n",
      "epoch 4, loss 0.39, time 40.20s\n",
      "epoch 5, loss 0.37, time 39.89s\n",
      "epoch 6, loss 0.35, time 40.54s\n",
      "epoch 7, loss 0.34, time 40.72s\n",
      "epoch 8, loss 0.33, time 42.60s\n",
      "epoch 9, loss 0.32, time 42.64s\n",
      "epoch 10, loss 0.32, time 42.79s\n"
     ]
    }
   ],
   "source": [
    "train(net, 0.01, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.weight - torch.Size([9858, 100])\n",
      "1.weight - torch.Size([9858, 100])\n"
     ]
    }
   ],
   "source": [
    "for name, param in net.named_parameters():\n",
    "    print(f\"{name} - {param.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.3.6 应用词嵌入模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine sim=1.000: chip\n",
      "cosine sim=0.422: seeing\n",
      "cosine sim=0.412: film\n",
      "cosine sim=0.399: computers\n"
     ]
    }
   ],
   "source": [
    "def get_similar_tokens(query_token, k, embed):\n",
    "    W = embed.weight.data\n",
    "    x = W[token_to_idx[query_token]]\n",
    "    # 添加的1e-9是为了数值稳定性\n",
    "    cos = torch.matmul(W, x) / (torch.sum(W * W, dim=1) * torch.sum(x * x) + 1e-9).sqrt()\n",
    "    _, topk = torch.topk(cos, k=k+1)\n",
    "    topk = topk.cpu().numpy()\n",
    "    for i in topk[:]:  # 除去输入词\n",
    "        print('cosine sim=%.3f: %s' % (cos[i], (idx_to_token[i])))\n",
    "        \n",
    "get_similar_tokens('chip', 3, net[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "man_idx = token_to_idx[\"man\"]\n",
    "woman_idx = token_to_idx[\"woman\"]\n",
    "father_idx = token_to_idx[\"father\"]\n",
    "mother_idx = token_to_idx[\"mather\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "man_vector = net[0].weight.data[man_idx]\n",
    "woman_vector = net[0].weight.data[woman_idx]\n",
    "father_vector = net[0].weight.data[father_idx]\n",
    "mother_vector = net[0].weight.data[mother_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = man_vector-woman_vector\n",
    "v2 = father_vector-mother_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos = torch.matmul(v1, v2) / (torch.sum(v1 * v1) * torch.sum(v2 * v2)).sqrt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1114)"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
